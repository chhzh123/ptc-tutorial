{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "    # (bs, head, seq, hs // head)\n",
    "    d_k = q.shape[-1]\n",
    "    attn_score = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(d_k)\n",
    "    # (bs, head, seq, seq)\n",
    "    attn_probs = F.softmax(attn_score, dim=-1)\n",
    "    attn_probs = F.dropout(attn_probs, 0.1)\n",
    "    # (bs, head, seq, hs // head)\n",
    "    attn = torch.matmul(attn_probs, v)\n",
    "    return attn\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def permute_for_scores(self, x):\n",
    "        # x: (batch_size, seq_len, hidden_size)\n",
    "        new_shape = x.shape[:-1] + (self.n_heads, -1)\n",
    "        x = x.view(new_shape)\n",
    "        # output: (bs, head, seq, hs // head)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: (batch_size, seq_len, hidden_size)\n",
    "        # qkv layers\n",
    "        q = self.permute_for_scores(self.q_proj(hidden_states))\n",
    "        k = self.permute_for_scores(self.k_proj(hidden_states))\n",
    "        v = self.permute_for_scores(self.v_proj(hidden_states))\n",
    "        # core attention\n",
    "        output = scaled_dot_product(q, k, v)\n",
    "        # output: (bs, seq, head, hs // head)\n",
    "        output.permute(0, 2, 1, 3)\n",
    "        output.view(output.shape[0], output.shape[1], -1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self, hidden_size, p=0.1):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.self_attn = SelfAttention(hidden_size, n_heads)\n",
    "        self.proj = Projection(hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        self_output = self.self_attn(hidden_states)\n",
    "        attention_output = self.proj(self_output, hidden_states)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, hidden_act):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.hidden_act = hidden_act\n",
    "        self.linear2 = nn.Linear(intermediate_size, hidden_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = self.linear1(data)\n",
    "        if self.hidden_act == \"gelu\":\n",
    "            out = F.gelu(out)\n",
    "        else:\n",
    "            out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchScript\n",
    "\n",
    "* First generation of PyTorch compiler\n",
    "* Can support both **training and inference**\n",
    "* Out-of-the-box optimiztaion tool\n",
    "\n",
    "PyTorch is a dynamic graph execution framework, so we need to firstly construct the computation graph in order to conduct further optimizations.\n",
    "\n",
    "Just-in-Time (JIT) compilation.\n",
    "\n",
    "* Tracing mode: `torch.jit.trace`\n",
    "* Scripting mode: `torch.jit.script`\n",
    "\n",
    "### Tracing Mode\n",
    "Runs a model with certain inputs and \"traces / records\" all the operations that are executed into a graph.\n",
    "\n",
    "We use the MLP example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f\"cuda:{torch.cuda.device_count() - 1}\"\n",
    "inp = torch.rand((16, 512, 768)).to(device) # (bs, seq, hs)\n",
    "mlp = MLP(768, 3072, \"gelu\").to(device)\n",
    "traced_mlp = torch.jit.trace(mlp, (inp,))\n",
    "print(traced_mlp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the **structural representation** that describes the module hierarchy. We can check the class type of the traced module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(traced_mlp), isinstance(traced_mlp, nn.Module))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the **graph representation** of the traced module, the intermediate representation (IR) mostly follows LLVM's convention.\n",
    "\n",
    "* Graph: Similar to `llvm::Function`\n",
    "* Block: Only dataflow is inside a block\n",
    "* Node: Instruction\n",
    "    * Analogous to `mlir::Operation`\n",
    "    * Can have nested blocks inside\n",
    "    * e.g., `prim::GetAttr`, `prim::CallMethod`, `prim::Constant`, `aten::gelu`\n",
    "* Value: Input arguments / Output results\n",
    "    * The edges in the graph\n",
    "    * Single-static assignment (SSA) form: Each value has precisely one defining node\n",
    "    * e.g., `%x: type` (statically typed!)\n",
    "\n",
    "You can refer to the implementation file [ir.h](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/ir/ir.h) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traced_mlp.graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even print out the executable Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traced_mlp.code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripting Mode\n",
    "\n",
    "Parses the Python source code of the model, and compiles the code into a graph.\n",
    "* a subset of Python grammar\n",
    "* Has a Lexer and Parser that parse Python syntax directly\n",
    "    * Useful to deploy to somewhere without Python environment (no need to link CPython)\n",
    "    * It cannot catch up the latest Python grammar, maintainability\n",
    "    * Limits the programmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_mlp = torch.jit.script(mlp)\n",
    "print(scripted_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(scripted_mlp), isinstance(scripted_mlp, nn.Module))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control flow nodes: `prim::If` and `prim::Loop`\n",
    "* Output of the if-statement serve a similar role to the $\\Phi$ node in traditional SSA control-flow graphs\n",
    "* Same as `mlir::affine::yield`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scripted_mlp.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scripted_mlp.code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operator fusion\n",
    "\n",
    "NVFuser\n",
    "\n",
    "https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/fused_bias_gelu.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code quality is the cost of scriptability**\n",
    "\n",
    "No transparency\n",
    "\n",
    "Note: do NOT use timeit or time in Python standard library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "print(benchmark.Timer('mlp(inp)', globals={'mlp': mlp, 'inp': inp}, label='Vanilla').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "print(benchmark.Timer('traced_mlp(inp)', globals={'traced_mlp': traced_mlp, 'inp': inp.detach().clone()}, label='Traced').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "print(benchmark.Timer('scripted_mlp(inp)', globals={'scripted_mlp': scripted_mlp, 'inp': inp.detach().clone()}, label='Scripted').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewMLP(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, hidden_act):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(intermediate_size))\n",
    "        self.hidden_act = hidden_act\n",
    "        self.linear2 = nn.Linear(intermediate_size, hidden_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = self.linear1(data)\n",
    "        out = out + self.bias\n",
    "        if self.hidden_act == \"gelu\":\n",
    "            out = F.gelu(out)\n",
    "        else:\n",
    "            out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = NewMLP(768, 3072, \"gelu\").to(device)\n",
    "traced_mlp = torch.jit.trace(mlp, (inp,))\n",
    "scripted_mlp = torch.jit.script(mlp)\n",
    "print(benchmark.Timer('mlp(inp)', globals={'mlp': mlp, 'inp': inp.detach().clone()}, label='Vanilla').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "print(benchmark.Timer('traced_mlp(inp)', globals={'traced_mlp': traced_mlp, 'inp': inp.detach().clone()}, label='Traced').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "print(benchmark.Timer('scripted_mlp(inp)', globals={'scripted_mlp': scripted_mlp, 'inp': inp.detach().clone()}, label='Scripted').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "static control flow\n",
    "does not related to data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation\n",
    "\n",
    "[JIT should not force users to write ugly code](https://github.com/pytorch/pytorch/issues/48108)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Middle None argument"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.fx\n",
    "\n",
    "MLSys paper (Horace He)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fx.symbolic_trace(mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
