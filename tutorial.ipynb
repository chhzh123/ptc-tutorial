{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial (Cornell ECE 6980)\n",
    "\n",
    "Authors: *Hongzheng Chen*, *Zhanqiu Hu*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! which python\n",
    "! python --version\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install torch numpy slapo tabulate transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"logs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Basics\n",
    "\n",
    "Construct the model in a hierarchical way.\n",
    "\n",
    "### Self-Attention\n",
    "\n",
    "$$\\mathrm{CoreAttention} \\left(Q, K, V\\right) = \\mathrm{softmax} \\left( \\frac{QK^\\mathrm{T}}{\\sqrt{d_k}} \\right) \\cdot V$$\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/transformer.png\" width=\"80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "    # (bs, head, seq, hs // head)\n",
    "    d_k = q.shape[-1]\n",
    "    attn_score = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(d_k)\n",
    "    # (bs, head, seq, seq)\n",
    "    attn_probs = F.softmax(attn_score, dim=-1)\n",
    "    attn_probs = F.dropout(attn_probs, 0.1)\n",
    "    # (bs, head, seq, hs // head)\n",
    "    attn = torch.matmul(attn_probs, v)\n",
    "    return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def permute_for_scores(self, x):\n",
    "        # x: (batch_size, seq_len, hidden_size)\n",
    "        new_shape = x.shape[:-1] + (self.n_heads, -1)\n",
    "        x = x.view(new_shape)\n",
    "        # output: (bs, head, seq, hs // head)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: (batch_size, seq_len, hidden_size)\n",
    "        # qkv layers\n",
    "        q = self.permute_for_scores(self.q_proj(hidden_states))\n",
    "        k = self.permute_for_scores(self.k_proj(hidden_states))\n",
    "        v = self.permute_for_scores(self.v_proj(hidden_states))\n",
    "        # core attention\n",
    "        output = scaled_dot_product(q, k, v)\n",
    "        # output: (bs, seq, head, hs // head)\n",
    "        output.permute(0, 2, 1, 3)\n",
    "        output.view(output.shape[0], output.shape[1], -1)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, intermediate_size, hidden_size, p=0.1):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.self_attn = SelfAttention(hidden_size, n_heads)\n",
    "        self.proj = Projection(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        self_output = self.self_attn(hidden_states)\n",
    "        attention_output = self.proj(self_output, hidden_states)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"Feed forward network (FFN) with GELU activation\"\"\"\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.activation = nn.GELU()\n",
    "        self.projection = Projection(intermediate_size, hidden_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = self.linear1(data)\n",
    "        out = self.activation(out)\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = Attention(hidden_size, intermediate_size, n_heads)\n",
    "        self.ffn = FFN(hidden_size, intermediate_size)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attention_output = self.attention(hidden_states)\n",
    "        ffn_output = self.ffn(attention_output)\n",
    "        return ffn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_layer = TransformerLayer(hidden_size=768, intermediate_size=3072, n_heads=12)\n",
    "print(transformer_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, BertLMHeadModel\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertLMHeadModel(config)\n",
    "print(config)\n",
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert_model.bert.encoder.layer[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device=\"cuda\", bs=8, seq_length=512, steps=40):\n",
    "    # data preparation\n",
    "    input_ids = torch.ones(bs, seq_length, dtype=torch.long, device=device)\n",
    "    attention_mask = torch.ones(bs, seq_length, dtype=torch.float32, device=device)\n",
    "    token_type_ids = torch.ones(bs, seq_length, dtype=torch.long, device=device)\n",
    "    labels = input_ids.clone()\n",
    "    # model preparation\n",
    "    model.to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    # training loop\n",
    "    for step in range(steps):\n",
    "        inputs = (input_ids, attention_mask, token_type_ids)\n",
    "        loss = model(*inputs, labels=labels).loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar(\"Loss/train\", loss, step)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"step {step} loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs\n",
    "train(bert_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchScript\n",
    "\n",
    "PyTorch uses **dynamic graph** representation (**eager mode** / define-by-run), which means the graph is built on-the-fly.\n",
    "\n",
    "\n",
    "> ðŸ’¡ **Graph mode** / define-and-run: TensorFlow, Caffe\n",
    "\n",
    "![](https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/dynamic_graph.gif)\n",
    "\n",
    "We need some ways to capture the dynamic graph into a static graph so that we can conduct more optimizations.\n",
    "\n",
    "### Just-in-Time (JIT) compilation\n",
    "\n",
    "![](https://d3i71xaburhd42.cloudfront.net/e99921410790e1876a6089d039a960a8ea3b3f66/3-Figure1-1.png)\n",
    "\n",
    "TorchScript\n",
    "* First generation of PyTorch compiler\n",
    "* Can support both **training and inference**\n",
    "* Out-of-the-box optimiztaion tool\n",
    "\n",
    "Two different modes:\n",
    "* Tracing mode: `torch.jit.trace`\n",
    "* Scripting mode: `torch.jit.script`\n",
    "\n",
    "### Tracing Mode\n",
    "\n",
    "Runs a model with certain inputs and \"traces / records\" all the operations that are executed into a graph.\n",
    "\n",
    "We use the MLP example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, hidden_act):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.hidden_act = hidden_act\n",
    "        self.linear2 = nn.Linear(intermediate_size, hidden_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = self.linear1(data)\n",
    "        if self.hidden_act == \"gelu\":\n",
    "            out = F.gelu(out)\n",
    "        else:\n",
    "            out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f\"cuda:{torch.cuda.device_count() - 1}\"\n",
    "inp = torch.rand((16, 512, 768)).to(device) # (bs, seq, hs)\n",
    "mlp = MLP(768, 3072, \"gelu\").to(device)\n",
    "traced_mlp = torch.jit.trace(mlp, (inp,))\n",
    "print(traced_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the **structural representation** that describes the module hierarchy. We can check the class type of the traced module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(traced_mlp), isinstance(traced_mlp, nn.Module))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the **graph representation** of the traced module, the intermediate representation (IR) mostly follows LLVM's convention.\n",
    "\n",
    "* Graph: Similar to `llvm::Function`\n",
    "* Block: Only dataflow is inside a block\n",
    "* Node: Instruction\n",
    "    * Analogous to `mlir::Operation`\n",
    "    * Can have nested blocks inside\n",
    "    * e.g., `prim::GetAttr`, `prim::CallMethod`, `prim::Constant`, `aten::gelu`\n",
    "* Value: Input arguments / Output results\n",
    "    * The edges in the graph\n",
    "    * Single-static assignment (SSA) form: Each value has precisely one defining node\n",
    "    * e.g., `%x: type` (statically typed!)\n",
    "\n",
    "You can refer to the implementation file [ir.h](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/ir/ir.h) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traced_mlp.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even print out the executable Python code from the TorchScript IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traced_mlp.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripting Mode\n",
    "\n",
    "Parses the Python source code of the model, and compiles the code into a graph.\n",
    "* A subset of Python grammar\n",
    "* Has a Lexer and Parser that parse Python syntax directly\n",
    "    * Useful to deploy to somewhere without Python environment (no need to link CPython)\n",
    "    * It cannot catch up with the latest Python grammar, poor maintainability\n",
    "    * Limits what can apply in the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_mlp = torch.jit.script(mlp)\n",
    "print(scripted_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use the Python decorator\n",
    "```python\n",
    "# decorate a class\n",
    "@torch.jit.script\n",
    "class MLP(nn.Module):\n",
    "    ...\n",
    "\n",
    "# decorate a function\n",
    "@torch.jit.script\n",
    "def foo(x):\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(scripted_mlp), isinstance(scripted_mlp, nn.Module))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control flow nodes: `prim::If` and `prim::Loop`\n",
    "* Output of the if-statement serve a similar role to the $\\Phi$ node in traditional SSA control-flow graphs\n",
    "* Same as `mlir::affine::yield`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scripted_mlp.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scripted_mlp.code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study: Operator fusion\n",
    "\n",
    "$g(f(x_1, \\cdots, x_n)) = (g\\circ f)(x_1, \\cdots, x_n)$\n",
    "\n",
    "* Reduce kernel launch overheads\n",
    "* Keep intermediate results in register instead of writing back to memory\n",
    "* TorchScript incorporates [NVFuser](https://pytorch.org/blog/introducing-nvfuser-a-deep-learning-compiler-for-pytorch/) as the backend fusion framework which is by default enabled\n",
    "\n",
    "We want to fuse the linear bias add and the GELU operation, since both are element-wise operations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ Do NOT use the standard Python library `timeit` or `time` to benchmark the PyTorch execution time on GPU. Otherwise `torch.cuda.synchronize()` is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "torch.cuda.empty_cache() # clear cache\n",
    "print(benchmark.Timer('mlp(inp)', globals={'mlp': mlp, 'inp': inp.detach().clone()}, label='Vanilla').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "print(benchmark.Timer('traced_mlp(inp)', globals={'traced_mlp': traced_mlp, 'inp': inp.detach().clone()}, label='Traced').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "print(benchmark.Timer('scripted_mlp(inp)', globals={'scripted_mlp': scripted_mlp, 'inp': inp.detach().clone()}, label='Scripted').timeit(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another attempt below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to modify somewhere...\n",
    "class NewMLP(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, hidden_act):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.hidden_act = hidden_act\n",
    "        self.linear2 = nn.Linear(intermediate_size, hidden_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        raise NotImplementedError\n",
    "        out = self.linear1(data)\n",
    "        if self.hidden_act == \"gelu\":\n",
    "            out = F.gelu(out)\n",
    "        else:\n",
    "            out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = NewMLP(768, 3072, \"gelu\").to(device)\n",
    "traced_mlp = torch.jit.trace(mlp, (inp,))\n",
    "scripted_mlp = torch.jit.script(mlp)\n",
    "torch.cuda.empty_cache()\n",
    "print(benchmark.Timer('mlp(inp)', globals={'mlp': mlp, 'inp': inp.detach().clone()}, label='Vanilla').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "print(benchmark.Timer('traced_mlp(inp)', globals={'traced_mlp': traced_mlp, 'inp': inp.detach().clone()}, label='Traced').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "print(benchmark.Timer('scripted_mlp(inp)', globals={'scripted_mlp': scripted_mlp, 'inp': inp.detach().clone()}, label='Scripted').timeit(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traced_mlp.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traced_mlp.graph_for(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scripted_mlp.graph_for(inp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ Prefer scripting a whole module rather than scripting a function, since scripting function only includes the forward pass.\n",
    ">\n",
    "> Check the implementation of Megatron-LM fused kernel: https://github.com/NVIDIA/Megatron-LM/blob/master/megatron/model/fused_bias_gelu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation\n",
    "\n",
    "> [JIT should not force users to write ugly code](https://github.com/pytorch/pytorch/issues/48108)\n",
    "\n",
    "* Generalization problem:\n",
    "    * Dynamic control flow: It is depended on the input data of the forward function\n",
    "    * Capture variables as constants (e.g., Dropout)\n",
    "* Only use basic syntax of Python: no/few custom structures, no builtins, no inheritance, no `Union`, no `**kwargs`, no `lambda`, no dynamic types, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dynamic control flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return torch.sqrt(x) if x.sum() > 0 else torch.square(x)\n",
    "m = torch.jit.trace(f, torch.tensor(3))\n",
    "print(m.code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Coding style not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L269-L277\n",
    "def foo(hidden_state, layer_past=None, attention_mask=None):\n",
    "    # do something\n",
    "    # ...\n",
    "    return hidden_state, layer_past, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_foo = torch.jit.trace(foo, (inp, None, inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_foo = torch.jit.script(foo)\n",
    "print(scripted_foo.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_foo(inp, None, inp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "\n",
    "* While optimization is done by a push button, code quality is the cost of scriptability and tracability.\n",
    "* No transparency on the optimizations. Compiler passes make code complicated and hard to debug.\n",
    "\n",
    "<!-- https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/fused_bias_gelu.py -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.fx\n",
    "\n",
    "> James K. Reed, Zachary DeVito, Horace He, Ansley Ussery, Jason Ansel, *[Torch.fx: Practical Program Capture and Transformation for Deep Learning in Python](https://arxiv.org/abs/2112.08429)*, MLSys, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import fx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design principles\n",
    "\n",
    "* Prefer making program capture and transformation easy for typical models at the cost of working for all possible programs. **Avoid complexity to support longtail**, esoteric use cases.\n",
    "* Work with tools and concepts that ML practitioners are already familiar with such as Python data structures and the publicly documented operators in PyTorch. (**Fully Pythonic**)\n",
    "* Make the process of program capture **highly configurable** so users can implement their own solutions for long-tail uses. Allowing users to make one-off configurations is simpler than handling the general case.\n",
    "\n",
    "> PyTorch is primarily used as an **eager execution** framework and program capture is only used for some specific transforms; It does not need to work for an entire program.\n",
    "> * [TorchDynamo](https://pytorch.org/docs/master/dynamo/): Only capture those can be captured and leave the rest to the Python native runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic tracing\n",
    "\n",
    "Use **abstract values (Proxy)** rather than example inputs.\n",
    "\n",
    "The static control flow is directly eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_traced_mlp = fx.symbolic_trace(mlp)\n",
    "print(fx_traced_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print out the graph IR, use `.graph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fx_traced_mlp.graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%name: [#users=x] = <node_type>[target=mod_or_func_name](args = (%x1,), kwargs = {...})`\n",
    "\n",
    "| Node type | Description |\n",
    "| :--: | :-- |\n",
    "| placeholder | input |\n",
    "| call_module | call a sub-`nn.Module` |\n",
    "| call_function | call a Python or PyTorch internal function (e.g., `operator.xxx`, `nn.functional.xxx`) |\n",
    "| call_method | call a class method |\n",
    "| get_attr | get a class attribute (e.g., parameter) |\n",
    "| output | return |\n",
    "\n",
    "* No primitive operations\n",
    "* `args` and `kwargs` support immediate values that are natively supported in Python\n",
    "* IR is much simpler\n",
    "\n",
    "![](https://d3i71xaburhd42.cloudfront.net/febc8c8018372f96867a7a56dc1b52cd682596c0/9-Figure5-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in fx_traced_mlp.graph.nodes:\n",
    "    print(node, node.op, node.target, node.args, node.kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_traced_mlp.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in fx_traced_mlp.graph.nodes:\n",
    "    if node.op == 'call_function' and node.target == F.gelu:\n",
    "        node.target = F.relu\n",
    "print(fx_traced_mlp.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in fx_traced_mlp.graph.nodes:\n",
    "    if node.op == 'call_module' and node.target == 'linear2': # string match\n",
    "        fx_traced_mlp.register_module('new_linear2', nn.Linear(3072, 3072, bias=False).to(device)) # be careful with the device\n",
    "        node.target = 'new_linear2'\n",
    "        break\n",
    "fx_traced_mlp.delete_all_unused_submodules()\n",
    "print(fx_traced_mlp.graph)\n",
    "# Need to recompile after modifying the graph\n",
    "fx_traced_mlp.graph.lint()\n",
    "fx_traced_mlp.recompile()\n",
    "print(fx_traced_mlp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert a node\n",
    "\n",
    "An incorrect implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in fx_traced_mlp.graph.nodes:\n",
    "    if node.op == 'call_module' and node.target == 'linear2':\n",
    "        with fx_traced_mlp.graph.inserting_after(node):\n",
    "            new_node = fx_traced_mlp.graph.call_function(F.relu, args=(node,))\n",
    "            node.replace_all_uses_with(new_node)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in fx_traced_mlp.graph.nodes:\n",
    "    if node.op == 'output':\n",
    "        with fx_traced_mlp.graph.inserting_before(node):\n",
    "            new_node = fx_traced_mlp.graph.call_function(F.relu, args=(node.args[0],))\n",
    "            node.args = (new_node,)\n",
    "        break\n",
    "fx_traced_mlp.graph.lint()\n",
    "fx_traced_mlp.recompile()\n",
    "print(fx_traced_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run code as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_traced_mlp(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/blob/master/torch/fx/passes/shape_prop.py\n",
    "from torch.fx.passes.shape_prop import ShapeProp\n",
    "\n",
    "ShapeProp(fx_traced_mlp).propagate(inp)\n",
    "for node in fx_traced_mlp.graph.nodes:\n",
    "    print(node, node.meta['tensor_meta'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "\n",
    "Need `pydot` to be installed.\n",
    "\n",
    "```python\n",
    "from torch.fx.passes.graph_drawer import FxGraphDrawer\n",
    "\n",
    "g = FxGraphDrawer(fx_traced_mlp, \"MLP\")\n",
    "g.get_main_dot_graph().create_svg()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check torch.fx [codebase](https://github.com/pytorch/pytorch/tree/master/torch/fx) to see more use cases. Also see fx [tutorial](https://pytorch.org/docs/stable/fx.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation\n",
    "\n",
    "* Dynamic control flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_to_trace(x):\n",
    "    if x.sum() > 0:\n",
    "        return torch.relu(x)\n",
    "    else:\n",
    "        return torch.neg(x)\n",
    "\n",
    "traced = torch.fx.symbolic_trace(func_to_trace)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Non-torch functions\n",
    "    * Use `wrap` to specify *leaf functions* that you do not want to trace into\n",
    "    * Similarly, [`Tracer`](https://github.com/pytorch/pytorch/blob/master/torch/fx/_symbolic_trace.py#L376) can be customized to have some *leaf_modules*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def normalize(x):\n",
    "    return x / sqrt(len(x))\n",
    "\n",
    "# It's valid Python code\n",
    "normalize(torch.rand(3, 4))\n",
    "\n",
    "traced = fx.symbolic_trace(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.fx.wrap('len')\n",
    "torch.fx.wrap('sqrt')\n",
    "\n",
    "traced = torch.fx.symbolic_trace(normalize)\n",
    "\n",
    "print(traced.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Full graph capturing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, BertLMHeadModel\n",
    "config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "model = BertLMHeadModel(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.fx as fx\n",
    "fx.symbolic_trace(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slapo\n",
    "\n",
    "### Challenges of both methods\n",
    "\n",
    "Compiler optimizations:\n",
    "* [C1] Programmability: All or nothing. Follow specific coding styles.\n",
    "* [C2] Debuggability: Hard to reason about the optimizations in a flattened optimized graph.\n",
    "\n",
    "Manual optimizations:\n",
    "* [C3] Generality: Need to modify the model definition or even rewrite the model.\n",
    "* [C4] Tunability: Tune for different configurations.\n",
    "\n",
    "### <u>S</u>chedule <u>LA</u>nguage for <u>P</u>rogressive <u>O</u>ptimization\n",
    "1. Decouple model schedule from definition [C3]\n",
    "2. Auto-tuner and auto-scheduler [C4]\n",
    "3. Progressive optimization with a \"trace-by-need\" approach [C1]\n",
    "4. Structure-preserved scheduling [C2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import slapo\n",
    "\n",
    "sch = slapo.create_schedule(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sch[\"bert.encoder.layer.0.attention\"].mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsch = sch[\"bert.encoder.layer.0.intermediate\"]\n",
    "print(subsch.mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operator Fusion (Dataflow Graph Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsch[\"dense\"].decompose()\n",
    "print(subsch.mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsch.trace(flatten=True)\n",
    "print(subsch.mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_pattern(bias, output):\n",
    "    return F.gelu(bias + output)\n",
    "subgraph = subsch.find(fusion_pattern)\n",
    "print(subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slapo.pattern import call_module\n",
    "\n",
    "# fuzzy matching\n",
    "def fusion_pattern(bias, output):\n",
    "    out = bias + output\n",
    "    out = call_module(r\"intermediate_.*\", out)\n",
    "    return out\n",
    "\n",
    "subgraph = subsch.find(fusion_pattern)\n",
    "print(subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsch.fuse(subgraph)\n",
    "print(subsch.mod)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Given the following subschedule, try to find and fuse the pattern: BiasAdd+Dropout+ResidualAdd+LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsch_out = subsch[\"bert.encoder.layer.0.output\"]\n",
    "# Your implementation\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module replacement (Structural Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_linear = nn.Linear(3072, 3072, bias=False).to(device)\n",
    "subsch[\"dense\"].replace(new_linear)\n",
    "print(subsch.mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization can also be achieved by module replacement.\n",
    "\n",
    "Check more primitives on Slapo's [documentation](https://awslabs.github.io/slapo/) webpage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 2.0\n",
    "\n",
    "Introduce the `torch.compile()` method to unify all the compilation techniques in the PyTorch ecosystem. See more on the [website](https://pytorch.org/get-started/pytorch-2.0/).\n",
    "\n",
    "* TorchDynamo as frontend: Only capture those can be captured and leave the rest to the Python native runtime\n",
    "* torch.fx as the mid-end IR\n",
    "* TorchInductor as the backend (+Triton, OpenMP, TVM, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
