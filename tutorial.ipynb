{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "    # (bs, head, seq, hs // head)\n",
    "    d_k = q.shape[-1]\n",
    "    attn_score = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(d_k)\n",
    "    # (bs, head, seq, seq)\n",
    "    attn_probs = F.softmax(attn_score, dim=-1)\n",
    "    attn_probs = F.dropout(attn_probs, 0.1)\n",
    "    # (bs, head, seq, hs // head)\n",
    "    attn = torch.matmul(attn_probs, v)\n",
    "    return attn\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def permute_for_scores(self, x):\n",
    "        # x: (batch_size, seq_len, hidden_size)\n",
    "        new_shape = x.shape[:-1] + (self.n_heads, -1)\n",
    "        x = x.view(new_shape)\n",
    "        # output: (bs, head, seq, hs // head)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: (batch_size, seq_len, hidden_size)\n",
    "        # qkv layers\n",
    "        q = self.permute_for_scores(self.q_proj(hidden_states))\n",
    "        k = self.permute_for_scores(self.k_proj(hidden_states))\n",
    "        v = self.permute_for_scores(self.v_proj(hidden_states))\n",
    "        # core attention\n",
    "        output = scaled_dot_product(q, k, v)\n",
    "        # output: (bs, seq, head, hs // head)\n",
    "        output.permute(0, 2, 1, 3)\n",
    "        output.view(output.shape[0], output.shape[1], -1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self, hidden_size, p=0.1):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.self_attn = SelfAttention(hidden_size, n_heads)\n",
    "        self.proj = Projection(hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        self_output = self.self_attn(hidden_states)\n",
    "        attention_output = self.proj(self_output, hidden_states)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, hidden_act):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.hidden_act = hidden_act\n",
    "        self.linear2 = nn.Linear(intermediate_size, hidden_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = self.linear1(data)\n",
    "        if self.hidden_act == \"gelu\":\n",
    "            out = F.gelu(out)\n",
    "        else:\n",
    "            out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchScript\n",
    "\n",
    "* First generation of PyTorch compiler\n",
    "* Can support both **training and inference**\n",
    "* Out-of-the-box optimiztaion tool\n",
    "\n",
    "PyTorch is a dynamic graph execution framework, so we need to firstly construct the computation graph in order to conduct further optimizations.\n",
    "\n",
    "Just-in-Time (JIT) compilation.\n",
    "\n",
    "* Tracing mode: `torch.jit.trace`\n",
    "* Scripting mode: `torch.jit.script`\n",
    "\n",
    "### Tracing Mode\n",
    "PyTorch uses **dynamic graph** representation (eager mode), which means the graph is built on-the-fly.\n",
    "\n",
    "![](https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/dynamic_graph.gif)\n",
    "\n",
    "Runs a model with certain inputs and \"traces / records\" all the operations that are executed into a graph.\n",
    "\n",
    "We use the MLP example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f\"cuda:{torch.cuda.device_count() - 1}\"\n",
    "inp = torch.rand((16, 512, 768)).to(device) # (bs, seq, hs)\n",
    "mlp = MLP(768, 3072, \"gelu\").to(device)\n",
    "traced_mlp = torch.jit.trace(mlp, (inp,))\n",
    "print(traced_mlp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the **structural representation** that describes the module hierarchy. We can check the class type of the traced module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(traced_mlp), isinstance(traced_mlp, nn.Module))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the **graph representation** of the traced module, the intermediate representation (IR) mostly follows LLVM's convention.\n",
    "\n",
    "* Graph: Similar to `llvm::Function`\n",
    "* Block: Only dataflow is inside a block\n",
    "* Node: Instruction\n",
    "    * Analogous to `mlir::Operation`\n",
    "    * Can have nested blocks inside\n",
    "    * e.g., `prim::GetAttr`, `prim::CallMethod`, `prim::Constant`, `aten::gelu`\n",
    "* Value: Input arguments / Output results\n",
    "    * The edges in the graph\n",
    "    * Single-static assignment (SSA) form: Each value has precisely one defining node\n",
    "    * e.g., `%x: type` (statically typed!)\n",
    "\n",
    "You can refer to the implementation file [ir.h](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/ir/ir.h) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traced_mlp.graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even print out the executable Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traced_mlp.code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripting Mode\n",
    "\n",
    "Parses the Python source code of the model, and compiles the code into a graph.\n",
    "* a subset of Python grammar\n",
    "* Has a Lexer and Parser that parse Python syntax directly\n",
    "    * Useful to deploy to somewhere without Python environment (no need to link CPython)\n",
    "    * It cannot catch up the latest Python grammar, maintainability\n",
    "    * Limits the programmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_mlp = torch.jit.script(mlp)\n",
    "print(scripted_mlp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use the Python decorator\n",
    "```python\n",
    "# decorate a class\n",
    "@torch.jit.script\n",
    "class MLP(nn.Module):\n",
    "    ...\n",
    "\n",
    "# decorate a function\n",
    "@torch.jit.script\n",
    "def foo(x):\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(scripted_mlp), isinstance(scripted_mlp, nn.Module))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control flow nodes: `prim::If` and `prim::Loop`\n",
    "* Output of the if-statement serve a similar role to the $\\Phi$ node in traditional SSA control-flow graphs\n",
    "* Same as `mlir::affine::yield`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scripted_mlp.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scripted_mlp.code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study: Operator fusion\n",
    "\n",
    "$g(f(x_1, \\cdots, x_n)) = g\\circ f(x_1, \\cdots, x_n)$\n",
    "\n",
    "* Reduce kernel launch overheads\n",
    "* TorchScript incorporates [NVFuser](https://pytorch.org/blog/introducing-nvfuser-a-deep-learning-compiler-for-pytorch/) as the backend fusion framework\n",
    "* NVFuser is by default enabled\n",
    "\n",
    "We want to fuse the linear bias add and the GELU operation, since both are element-wise operations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Do NOT use `timeit` or `time` in the standard Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "print(benchmark.Timer('mlp(inp)', globals={'mlp': mlp, 'inp': inp}, label='Vanilla').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "print(benchmark.Timer('traced_mlp(inp)', globals={'traced_mlp': traced_mlp, 'inp': inp.detach().clone()}, label='Traced').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "print(benchmark.Timer('scripted_mlp(inp)', globals={'scripted_mlp': scripted_mlp, 'inp': inp.detach().clone()}, label='Scripted').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another attempt below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewMLP(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, hidden_act):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(intermediate_size))\n",
    "        self.hidden_act = hidden_act\n",
    "        self.linear2 = nn.Linear(intermediate_size, hidden_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = self.linear1(data)\n",
    "        out = out + self.bias\n",
    "        if self.hidden_act == \"gelu\":\n",
    "            out = F.gelu(out)\n",
    "        else:\n",
    "            out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = NewMLP(768, 3072, \"gelu\").to(device)\n",
    "traced_mlp = torch.jit.trace(mlp, (inp,))\n",
    "scripted_mlp = torch.jit.script(mlp)\n",
    "print(benchmark.Timer('mlp(inp)', globals={'mlp': mlp, 'inp': inp.detach().clone()}, label='Vanilla').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "print(benchmark.Timer('traced_mlp(inp)', globals={'traced_mlp': traced_mlp, 'inp': inp.detach().clone()}, label='Traced').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "print(benchmark.Timer('scripted_mlp(inp)', globals={'scripted_mlp': scripted_mlp, 'inp': inp.detach().clone()}, label='Scripted').timeit(1000))\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traced_mlp.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traced_mlp.graph_for(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scripted_mlp.graph_for(inp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Prefer scripting a whole module more than scripting a function, since scripting function only includes the forward pass."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation\n",
    "\n",
    "> [JIT should not force users to write ugly code](https://github.com/pytorch/pytorch/issues/48108)\n",
    "\n",
    "* Generalization problem:\n",
    "    * Dynamic control flow: It is depended on the input data of the forward function\n",
    "    * Capture variables as constants (e.g., Dropout)\n",
    "* Only use basic syntax of Python: no/few custom structures, no builtins, no inheritance, no Union, no **kwargs, no lambda, no dynamic types, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return torch.sqrt(x) if x.sum() > 0 else torch.square(x)\n",
    "m = torch.jit.trace(f, torch.tensor(3))\n",
    "print(m.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L269-L277\n",
    "def foo(hidden_state, layer_past=None, attention_mask=None):\n",
    "    # do something\n",
    "    # ...\n",
    "    return hidden_state, layer_past, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_foo = torch.jit.trace(foo, (inp, None, inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_foo = torch.jit.script(foo)\n",
    "print(scripted_foo.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_foo(inp, None, inp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaway:\n",
    "* Code quality is the cost of scriptability and tracability.\n",
    "* No transparency on the optimizations. Compiler passes make code complicated and hard to debug.\n",
    "\n",
    "<!-- https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/fused_bias_gelu.py -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.fx\n",
    "\n",
    "> James K. Reed, Zachary DeVito, Horace He, Ansley Ussery, Jason Ansel, *[Torch.fx: Practical Program Capture and Transformation for Deep Learning in Python](https://arxiv.org/abs/2112.08429)*, MLSys, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import fx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design principles:\n",
    "* Prefer making program capture and transformation easy for typical models at the cost of working for all possible programs. Avoid complexity to support longtail, esoteric use cases.\n",
    "* Work with tools and concepts that ML practitioners are already familiar with such as Python data structures and the publicly documented operators in PyTorch. (Fully Pythonic)\n",
    "* Make the process of program capture highly configurable so users can implement their own solutions for long-tail uses. Allowing users to make one-off configurations is simpler than handling the general case.\n",
    "\n",
    "> PyTorch is primarily used as an **eager execution** framework and program capture is only used for some specific transforms; It does not need to work for an entire program."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Symbolic tracing**: Use abstract values (Proxy) rather than example inputs\n",
    "\n",
    "The static control flow is directly eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_traced_mlp = fx.symbolic_trace(mlp)\n",
    "print(fx_traced_mlp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print out the graph IR, use `.graph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fx_traced_mlp.graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%name: [#users=x] = <node_type>[target=mod_or_func_name](args = (%x1,), kwargs = {...})`\n",
    "\n",
    "| Node type | Description |\n",
    "| :--: | :-- |\n",
    "| placeholder | input |\n",
    "| call_module | call a sub-`nn.Module` |\n",
    "| call_function | call a Python or PyTorch internal function (e.g., `operator.xxx`, `nn.functional.xxx`) |\n",
    "| call_method | call a class method |\n",
    "| get_attr | get a class attribute (e.g., parameter) |\n",
    "| output | return |\n",
    "\n",
    "* No primitive operations\n",
    "* `args` and `kwargs` support immediate values that are natively supported in Python\n",
    "* IR is much simpler\n",
    "\n",
    "![](fig/fx_IR_comparison.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in fx_traced_mlp.graph.nodes:\n",
    "    print(node, node.op, node.target, node.args, node.kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_traced_mlp.graph.print_tabular()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Manipulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in fx_traced_mlp.graph.nodes:\n",
    "    if node.op == 'call_function' and node.target == F.gelu:\n",
    "        node.target = F.relu\n",
    "print(fx_traced_mlp.graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in fx_traced_mlp.graph.nodes:\n",
    "    if node.op == 'call_module' and node.target == 'linear2': # string match\n",
    "        fx_traced_mlp.register_module('new_linear2', nn.Linear(3072, 3072, bias=False).to(device)) # be careful with the device\n",
    "        node.target = 'new_linear2'\n",
    "        break\n",
    "fx_traced_mlp.delete_all_unused_submodules()\n",
    "print(fx_traced_mlp.graph)\n",
    "# Need to recompile after modifying the graph\n",
    "fx_traced_mlp.graph.lint()\n",
    "fx_traced_mlp.recompile()\n",
    "print(fx_traced_mlp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in fx_traced_mlp.graph.nodes:\n",
    "    if node.op == 'output':\n",
    "        with fx_traced_mlp.graph.inserting_before(node):\n",
    "            new_node = fx_traced_mlp.graph.call_function(F.relu, args=(node.args[0],))\n",
    "            node.args = (new_node,)\n",
    "        break\n",
    "fx_traced_mlp.graph.lint()\n",
    "fx_traced_mlp.recompile()\n",
    "print(fx_traced_mlp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run code as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_traced_mlp(inp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/blob/master/torch/fx/passes/shape_prop.py\n",
    "from torch.fx.passes.shape_prop import ShapeProp\n",
    "\n",
    "ShapeProp(fx_traced_mlp).propagate(inp)\n",
    "for node in fx_traced_mlp.graph.nodes:\n",
    "    print(node, node.meta['tensor_meta'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "\n",
    "Need `pydot` to be installed.\n",
    "\n",
    "```python\n",
    "from torch.fx.passes.graph_drawer import FxGraphDrawer\n",
    "\n",
    "g = FxGraphDrawer(fx_traced_mlp, \"MLP\")\n",
    "g.get_main_dot_graph().create_svg()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check torch.fx [codebase](https://github.com/pytorch/pytorch/tree/master/torch/fx) to see more use cases. Also see fx [tutorial](https://pytorch.org/docs/stable/fx.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation\n",
    "\n",
    "* Dynamic control flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_to_trace(x):\n",
    "    if x.sum() > 0:\n",
    "        return torch.relu(x)\n",
    "    else:\n",
    "        return torch.neg(x)\n",
    "\n",
    "traced = torch.fx.symbolic_trace(func_to_trace)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Non-torch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def normalize(x):\n",
    "    return x / sqrt(len(x))\n",
    "\n",
    "# It's valid Python code\n",
    "normalize(torch.rand(3, 4))\n",
    "\n",
    "traced = fx.symbolic_trace(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.fx.wrap('len')\n",
    "torch.fx.wrap('sqrt')\n",
    "\n",
    "traced = torch.fx.symbolic_trace(normalize)\n",
    "\n",
    "print(traced.code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, [`Tracer`](https://github.com/pytorch/pytorch/blob/master/torch/fx/_symbolic_trace.py#L376) can be customized to have some *leaf_modules*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
