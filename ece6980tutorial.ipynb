{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from transformers import BertLMHeadModel, AutoConfig\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Pytroch model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Self-Attention\n",
    "$Attention \\left(Q, K, V\\right) = softmax \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) \\cdot V$\n",
    "\n",
    "<!-- ![alternatvie text](attention.png) -->\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"attention.png\" height=\"400\"/>\n",
    "<img src=\"multihead.png\" height=\"400\"/>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "    # (bs, head, seq, hs // head)\n",
    "    d_k = q.shape[-1]\n",
    "    attn_score = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(d_k)\n",
    "    # (bs, head, seq, seq)\n",
    "    attn_probs = F.softmax(attn_score, dim=-1)\n",
    "    attn_probs = F.dropout(attn_probs, 0.1)\n",
    "    # (bs, head, seq, hs // head)\n",
    "    attn = torch.matmul(attn_probs, v)\n",
    "    return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def permute_for_scores(self, x):\n",
    "        # x: (batch_size, seq_len, hidden_size)\n",
    "        new_shape = x.shape[:-1] + (self.n_heads, -1)\n",
    "        x = x.view(new_shape)\n",
    "        # output: (bs, head, seq, hs // head)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: (batch_size, seq_len, hidden_size)\n",
    "        # qkv layers\n",
    "        q = self.permute_for_scores(self.q_proj(hidden_states))\n",
    "        k = self.permute_for_scores(self.k_proj(hidden_states))\n",
    "        v = self.permute_for_scores(self.v_proj(hidden_states))\n",
    "        # core attention\n",
    "        output = scaled_dot_product(q, k, v)\n",
    "        # output: (bs, seq, head, hs // head)\n",
    "        output.permute(0, 2, 1, 3)\n",
    "        output.view(output.shape[0], output.shape[1], -1)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention Layer\n",
    "<div>\n",
    "<center>\n",
    "<img src=\"transformer.png\" width=\"400\"/>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.GELU()\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = self.linear1(data)\n",
    "        out = self.activation(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_heads):\n",
    "        super().__init__()\n",
    "        self.self_attn = SelfAttention(hidden_size, n_heads)\n",
    "        self.proj1 = Projection(hidden_size)\n",
    "        self.linear_net = MLP(hidden_size)\n",
    "        self.proj2 = Projection(hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        self_output = self.self_attn(hidden_states)\n",
    "        attention_output = self.proj1(self_output, hidden_states)\n",
    "        linear_out = self.linear_net(attention_output)\n",
    "        linear_out = attention_output + self.dropout(linear_out)\n",
    "        out = self.proj2(linear_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention(\n",
      "  (self_attn): SelfAttention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (proj1): Projection(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (linear_net): MLP(\n",
      "    (linear1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (activation): GELU(approximate=none)\n",
      "    (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (proj2): Projection(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Attention(hidden_size=1024, n_heads=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"bert-large-uncased\")\n",
    "bert_model = BertLMHeadModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bert_model.bert.encoder.layer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device=\"cuda\", bs=8, seq_length=512):\n",
    "    input_ids = torch.ones(bs, seq_length, dtype=torch.long, device=device)\n",
    "    attention_mask = torch.ones(bs, seq_length, dtype=torch.float16, device=device)\n",
    "    token_type_ids = torch.ones(bs, seq_length, dtype=torch.long, device=device)\n",
    "    labels = input_ids.clone()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    for step in range(100):\n",
    "        inputs = (input_ids, attention_mask, token_type_ids)\n",
    "        loss = model(*inputs, labels=labels).loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"step {step} loss: {loss.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a simple model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, num_classes),\n",
    "            #nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "model = Model(num_classes = 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist(batch_size: int,\n",
    "              data_dir: str = '/tmp/mnist_data'):\n",
    "        normalize = transforms.Normalize(mean=[0.1307], std=[0.3081])\n",
    "        transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "        trainset = torchvision.datasets.MNIST(\n",
    "            root=data_dir, train=True, transform=transform, download=True)\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "            trainset, batch_size=batch_size, shuffle=True)\n",
    "        testset = torchvision.datasets.MNIST(\n",
    "            root=data_dir, train=False, transform=transform, download=True)\n",
    "        testloader = torch.utils.data.DataLoader(\n",
    "            testset, batch_size=batch_size, shuffle=False)\n",
    "        return trainloader, testloader\n",
    "\n",
    "trainloader, testloader = mnist(batch_size = 512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick a optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(model, init_lr, weight_decay=0.0):\n",
    "    optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=init_lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=weight_decay)\n",
    "    return optimizer\n",
    "\n",
    "def sgd(model, init_lr, weight_decay=0.0):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=init_lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=weight_decay)\n",
    "        return optimizer\n",
    "\n",
    "optimizer = sgd(model, 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick a scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multistep(optimizer, milestones, gamma=0.1):\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=gamma)\n",
    "    return scheduler\n",
    "\n",
    "scheduler = multistep(optimizer, [5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer = SummaryWriter()\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() \n",
    "          else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "def test(model, testloader=testloader, device=device):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            # measure accuracy and record loss\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct * 100 / total\n",
    "\n",
    "def training_loop(model, optimizer, scheduler, num_epochs, device=device):\n",
    "    global_step = 0\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # set printing functions\n",
    "        \n",
    "        # switch the model to the training mode\n",
    "        model.train()\n",
    "\n",
    "        tb_writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "        # each epoch\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            acc = test(model, trainloader)\n",
    "            accuracies.append(acc)\n",
    "            \n",
    "            tb_writer.add_scalar('Loss/train', loss, global_step)\n",
    "            tb_writer.add_scalar('Acc/train', acc, global_step)\n",
    "            print(f\"Training loss at step {global_step} is: {loss}\")\n",
    "            global_step += 1\n",
    "\n",
    "        # update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # evaluate the model every few epochs\n",
    "        \n",
    "        acc = test(model)\n",
    "        print(f\"Test acc at step {global_step} is: {acc}\")\n",
    "        \n",
    "        tb_writer.add_scalar('Acc/eval', acc, global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 0 is: 2.3106632232666016\n",
      "Training loss at step 1 is: 2.303093433380127\n",
      "Training loss at step 2 is: 2.2949700355529785\n",
      "Training loss at step 3 is: 2.2950339317321777\n",
      "Training loss at step 4 is: 2.289426803588867\n",
      "Training loss at step 5 is: 2.286762237548828\n",
      "Training loss at step 6 is: 2.2789368629455566\n",
      "Training loss at step 7 is: 2.2720489501953125\n",
      "Training loss at step 8 is: 2.2601583003997803\n",
      "Training loss at step 9 is: 2.244412422180176\n",
      "Training loss at step 10 is: 2.224745988845825\n",
      "Training loss at step 11 is: 2.1894540786743164\n",
      "Training loss at step 12 is: 2.1639657020568848\n",
      "Training loss at step 13 is: 2.0337555408477783\n",
      "Training loss at step 14 is: 1.9824663400650024\n",
      "Training loss at step 15 is: 1.7275285720825195\n",
      "Training loss at step 16 is: 1.5443382263183594\n",
      "Training loss at step 17 is: 1.4508697986602783\n",
      "Training loss at step 18 is: 5.747685432434082\n",
      "Training loss at step 19 is: 4.923940658569336\n",
      "Training loss at step 20 is: 2.272399663925171\n",
      "Training loss at step 21 is: 2.318662166595459\n",
      "Training loss at step 22 is: 2.32656192779541\n",
      "Training loss at step 23 is: 2.3895092010498047\n",
      "Training loss at step 24 is: 2.340665817260742\n",
      "Training loss at step 25 is: 2.3162479400634766\n",
      "Training loss at step 26 is: 2.279165029525757\n",
      "Training loss at step 27 is: 2.2442965507507324\n",
      "Training loss at step 28 is: 2.2223763465881348\n",
      "Training loss at step 29 is: 2.1793770790100098\n",
      "Training loss at step 30 is: 2.172746181488037\n",
      "Training loss at step 31 is: 2.0646047592163086\n",
      "Training loss at step 32 is: 2.137753486633301\n",
      "Training loss at step 33 is: 2.1082987785339355\n",
      "Training loss at step 34 is: 2.0844976902008057\n",
      "Training loss at step 35 is: 2.06897234916687\n",
      "Training loss at step 36 is: 2.0599753856658936\n",
      "Training loss at step 37 is: 2.019761085510254\n",
      "Training loss at step 38 is: 1.9286714792251587\n",
      "Training loss at step 39 is: 1.9112660884857178\n",
      "Training loss at step 40 is: 1.7214186191558838\n",
      "Training loss at step 41 is: 1.6705111265182495\n",
      "Training loss at step 42 is: 1.4548701047897339\n",
      "Training loss at step 43 is: 1.4023829698562622\n",
      "Training loss at step 44 is: 2.170757532119751\n",
      "Training loss at step 45 is: 2.849148750305176\n",
      "Training loss at step 46 is: 2.3898873329162598\n",
      "Training loss at step 47 is: 2.367884874343872\n",
      "Training loss at step 48 is: 2.323253870010376\n",
      "Training loss at step 49 is: 2.2829744815826416\n",
      "Training loss at step 50 is: 2.3197226524353027\n",
      "Training loss at step 51 is: 2.1974024772644043\n",
      "Training loss at step 52 is: 2.1263716220855713\n",
      "Training loss at step 53 is: 2.062450408935547\n",
      "Training loss at step 54 is: 2.0250205993652344\n",
      "Training loss at step 55 is: 2.0051841735839844\n",
      "Training loss at step 56 is: 2.027631998062134\n",
      "Training loss at step 57 is: 1.9855916500091553\n",
      "Training loss at step 58 is: 1.892959713935852\n",
      "Training loss at step 59 is: 1.7852823734283447\n",
      "Training loss at step 60 is: 1.7223381996154785\n",
      "Training loss at step 61 is: 1.6869947910308838\n",
      "Training loss at step 62 is: 1.497133493423462\n",
      "Training loss at step 63 is: 1.5513216257095337\n",
      "Training loss at step 64 is: 1.268462061882019\n",
      "Training loss at step 65 is: 1.3218028545379639\n",
      "Training loss at step 66 is: 1.106947422027588\n",
      "Training loss at step 67 is: 1.694575548171997\n",
      "Training loss at step 68 is: 1.216748833656311\n",
      "Training loss at step 69 is: 1.098639726638794\n",
      "Training loss at step 70 is: 1.4009414911270142\n",
      "Training loss at step 71 is: 1.3103201389312744\n",
      "Training loss at step 72 is: 1.193872094154358\n",
      "Training loss at step 73 is: 1.1528513431549072\n",
      "Training loss at step 74 is: 0.9907245635986328\n",
      "Training loss at step 75 is: 1.0259275436401367\n",
      "Training loss at step 76 is: 1.0678596496582031\n",
      "Training loss at step 77 is: 1.0533673763275146\n",
      "Training loss at step 78 is: 0.9204229116439819\n",
      "Training loss at step 79 is: 0.9481955766677856\n",
      "Training loss at step 80 is: 0.7367285490036011\n",
      "Training loss at step 81 is: 0.6822227239608765\n",
      "Training loss at step 82 is: 0.6637809872627258\n",
      "Training loss at step 83 is: 0.8032374382019043\n",
      "Training loss at step 84 is: 0.6889452934265137\n",
      "Training loss at step 85 is: 0.6384820938110352\n",
      "Training loss at step 86 is: 0.5996148586273193\n",
      "Training loss at step 87 is: 0.48963871598243713\n",
      "Training loss at step 88 is: 0.649994969367981\n",
      "Training loss at step 89 is: 0.5840497016906738\n",
      "Training loss at step 90 is: 0.5504072904586792\n",
      "Training loss at step 91 is: 0.6239879727363586\n",
      "Training loss at step 92 is: 0.7347008585929871\n",
      "Training loss at step 93 is: 0.4253321886062622\n",
      "Training loss at step 94 is: 0.6570398807525635\n",
      "Training loss at step 95 is: 0.5632349252700806\n",
      "Training loss at step 96 is: 0.566470742225647\n",
      "Training loss at step 97 is: 0.6562097072601318\n",
      "Training loss at step 98 is: 0.623687744140625\n",
      "Training loss at step 99 is: 0.5276917815208435\n",
      "Training loss at step 100 is: 0.6414206027984619\n",
      "Training loss at step 101 is: 0.553189754486084\n",
      "Training loss at step 102 is: 0.45921435952186584\n",
      "Training loss at step 103 is: 0.5312519073486328\n",
      "Training loss at step 104 is: 0.5791800618171692\n",
      "Training loss at step 105 is: 0.48662227392196655\n",
      "Training loss at step 106 is: 0.5059210658073425\n",
      "Training loss at step 107 is: 0.4947448670864105\n",
      "Training loss at step 108 is: 0.4567109942436218\n",
      "Training loss at step 109 is: 0.496432363986969\n",
      "Training loss at step 110 is: 0.40199536085128784\n",
      "Training loss at step 111 is: 0.4508891701698303\n",
      "Training loss at step 112 is: 0.40978744626045227\n",
      "Training loss at step 113 is: 0.471689373254776\n",
      "Training loss at step 114 is: 0.4994875490665436\n",
      "Training loss at step 115 is: 0.5445621013641357\n",
      "Training loss at step 116 is: 0.4083595871925354\n",
      "Training loss at step 117 is: 0.35539472103118896\n",
      "Training loss at step 118 is: 0.3588615357875824\n",
      "Training loss at step 119 is: 0.42770707607269287\n",
      "Training loss at step 120 is: 0.37322551012039185\n",
      "Training loss at step 121 is: 0.6297882795333862\n",
      "Training loss at step 122 is: 0.43881240487098694\n",
      "Training loss at step 123 is: 0.3516566753387451\n",
      "Training loss at step 124 is: 0.36042118072509766\n",
      "Training loss at step 125 is: 0.4217487573623657\n",
      "Training loss at step 126 is: 0.2984012961387634\n",
      "Training loss at step 127 is: 0.33665430545806885\n",
      "Training loss at step 128 is: 0.5128653049468994\n",
      "Training loss at step 129 is: 0.5030250549316406\n",
      "Training loss at step 130 is: 0.304453045129776\n",
      "Training loss at step 131 is: 0.27035069465637207\n",
      "Training loss at step 132 is: 0.3989390730857849\n",
      "Training loss at step 133 is: 0.39205849170684814\n",
      "Training loss at step 134 is: 0.4837746024131775\n",
      "Training loss at step 135 is: 0.3373057246208191\n",
      "Training loss at step 136 is: 0.30288761854171753\n",
      "Training loss at step 137 is: 0.44228941202163696\n",
      "Training loss at step 138 is: 0.30390167236328125\n",
      "Training loss at step 139 is: 0.32277601957321167\n"
     ]
    }
   ],
   "source": [
    "training_loop(model, optimizer, scheduler, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
